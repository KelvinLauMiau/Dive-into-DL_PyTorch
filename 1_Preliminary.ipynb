{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95df0f5672e2824a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 预备知识 · 数据操作 & 自动求梯度\n",
    "\n",
    "不得不开始好好学 pytorch ，那么开始，首先是 tensor 张量\n",
    "\n",
    "那么在 tensor 之前先复习下 numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T15:07:21.922911Z",
     "start_time": "2024-04-18T15:07:21.793842300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D Array: [1 2 3 4 5]\n",
      "2D Array:\n",
      " [[1 2 3]\n",
      " [4 5 6]]\n",
      "Array Shape: (2, 3)\n",
      "Number of dimensions: 2\n",
      "Array size: 6\n",
      "Data type: int32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一维数组\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "print(\"1D Array:\", arr)\n",
    "\n",
    "# 创建二维数组\n",
    "arr_2d = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"2D Array:\\n\", arr_2d)\n",
    "\n",
    "print(\"Array Shape:\", arr_2d.shape)  # 形状\n",
    "print(\"Number of dimensions:\", arr_2d.ndim)  # 维度数\n",
    "print(\"Array size:\", arr_2d.size)  # 数组大小\n",
    "print(\"Data type:\", arr_2d.dtype)  # 数据类型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151cd071f811100",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Q: C 里面有自己的数组，那 Python 有自己的数组吗，为什么一定要 numpy 来操作 np.array 数组，而不直接用 Python 的数组?相应的，为什么 tensor 要用 numpy 的数组，而不是直接用 Python 的数组?\n",
    "\n",
    "A: 在Python中，确实有一个内置的数组模块叫做array，它可以创建存储单一数据类型的紧凑数组。然而，Python的array模块相比于NumPy提供的ndarray功能来说较为简单，主要用于基础的数组存储，没有NumPy那么强大的功能集合，特别是在进行科学计算时。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d9edd772d8c2fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T15:10:27.047011300Z",
     "start_time": "2024-04-18T15:10:27.009098100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 6]\n",
      "[ 2  4  6  8 10]\n",
      "[ 1  4  9 16 25]\n",
      "2\n",
      "3\n",
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "# 索引和切片\n",
    "\n",
    "# 加法\n",
    "print(arr + 1)\n",
    "\n",
    "# 乘法\n",
    "print(arr * 2)\n",
    "\n",
    "# 平方\n",
    "print(arr ** 2)\n",
    "\n",
    "# 获取第二个元素\n",
    "print(arr[1])\n",
    "\n",
    "# 获取第一行的第三列元素\n",
    "print(arr_2d[0, 2])\n",
    "\n",
    "# 切片：获取前两个元素\n",
    "print(arr[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb0048c70a7543b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Q: 为什么Tensor要使用NumPy数组，而非Python原生数组？\n",
    "\n",
    "在深度学习中，数据操作通常需要处理大规模的数据集和执行复杂的数值计算，这些计算往往需要高效的数据结构支持：\n",
    "\n",
    "计算优化：像TensorFlow和PyTorch这样的深度学习框架使用的Tensor对象在内部优化了数据存储和计算，使得在GPU或其他硬件加速器上运行时可以获得更高的性能。\n",
    "\n",
    "自动求导：在深度学习中，自动求导是一个核心功能，它允许自动计算神经网络中参数的梯度。使用Python的原生数据类型如列表或数组来实现这种复杂的功能会非常困难和低效。\n",
    "\n",
    "与硬件的集成：深度学习框架需要与底层硬件如GPU紧密集成，以利用其并行计算能力。NumPy和Tensor对象的设计允许它们更好地与这些硬件技术集成。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4cf61a6cfca3028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T15:12:50.349986200Z",
     "start_time": "2024-04-18T15:12:50.322960400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1072e+20, 1.8007e-42, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.empty(5, 3)\n",
    "\n",
    "x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96705fb96c5044c8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\"tensor\"这个单词一般可译作“张量”，张量可以看作是一个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。\n",
    "\n",
    "上面即创建了一个 5x3 的未初始化 tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e813f36b29f13f43",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5 x 3 的随机tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e781cab5acba27ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T15:12:21.318575800Z",
     "start_time": "2024-04-18T15:12:21.270390500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5443, 0.8283, 0.1795],\n",
       "        [0.4487, 0.9190, 0.7166],\n",
       "        [0.3953, 0.0502, 0.5057],\n",
       "        [0.6909, 0.7040, 0.0336],\n",
       "        [0.7739, 0.0754, 0.3328]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980a2a2f39d3d75",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Q: 为什么 x = torch.empty(5, 3)  和  x = torch.rand(5, 3) 经最后print(x) 查看都是随机的\n",
    "\n",
    "在PyTorch中，创建张量（tensors）时有几种不同的方式来初始化它们，其中torch.empty()和torch.rand()是两个常用的方法。这两个函数都会生成一个具有指定维度的张量，但它们的初始化内容和用途有所不同。\n",
    "\n",
    "1. torch.empty(size):\n",
    "\n",
    "    torch.empty()创建一个未初始化的张量。这意味着在张量的内存分配后，不会对张量中的数据进行初始化处理。因此，torch.empty()返回的张量将包含任何时刻内存空间中的数据（垃圾数据）。所以，你看到的数据看起来是随机的，但实际上它们是内存中之前留下的值。\n",
    "   \n",
    "    \n",
    "    使用torch.empty()的主要理由是初始化速度非常快，如果你确保之后立即以某种方式填充它（例如，将其用作某些运算的输出缓存），那么使用它是安全的。\n",
    "\n",
    "3. torch.rand(size):\n",
    "\n",
    "    torch.rand()创建一个张量，并用[0, 1)范围内的均匀分布的随机数初始化这个张量。这意味着每个元素都是随机生成的，遵循均匀分布。\n",
    "   \n",
    "\n",
    "    这种方法通常用于初始化需要随机数的场景，如在训练神经网络时初始化权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5975f222139d8cf4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5 x 4 的 long 型全 0 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb8abf09044eb56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T15:13:01.517327Z",
     "start_time": "2024-04-18T15:13:01.484008600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5, 4, dtype=torch.long)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4a1c4cd414cd2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "直接根据数据创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0de35f329e2383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T15:13:28.270049300Z",
     "start_time": "2024-04-18T15:13:28.219878900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79814c67ade3f30b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "通过现有的 tensor 来创建，会默认重用输入 tensor 的一些属性，例如数据类型，除非自定义数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a6ef44096b22ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:41.825201600Z",
     "start_time": "2024-04-15T08:58:41.765533Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d74e6b9837a681",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "x.new_ones() 方法是一个便利函数，用于基于现有张量x的属性（比如设备和维度）创建一个新的张量。新张量的所有元素都被设置为1。这里的5, 3指定了新张量的形状，即5行3列。\n",
    "\n",
    "\n",
    "dtype=torch.float64 指定了新张量的数据类型为64位浮点数（即双精度浮点数）。如果你没有提供一个基张量x，你需要使用其他函数（如torch.ones）来创建类似的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd6f195b529026f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:41.826209200Z",
     "start_time": "2024-04-15T08:58:41.780536200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4324, -1.1562,  0.9556],\n",
      "        [ 0.6745,  1.2508,  0.6208],\n",
      "        [-0.9745, -0.2765,  0.0776],\n",
      "        [-1.3281,  1.2978, -1.2698],\n",
      "        [ 0.1111, -0.0748, -0.3512]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型\n",
    "print(x) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c878fdfbf6de09c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "torch.randn_like() 方法基于提供的张量x的形状和设备属性创建一个新的张量，但用标准正态分布（均值0，方差1的高斯分布）中的随机数填充它。\n",
    "\n",
    "dtype=torch.float 修改了数据类型为32位浮点数（即单精度浮点数）。这意味着，尽管原始的x张量是双精度的，这个调用生成的新张量将具有不同的数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bb8fff844d3ee35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:41.827162100Z",
     "start_time": "2024-04-15T08:58:41.796696400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bd4766d17b5b5d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "下面是一些常见的常用创建指令参考\n",
    "\n",
    "| 函数 | 功能 |\n",
    "| --- | --- |\n",
    "| `Tensor(*sizes)` | 直接构造指定大小的张量 |\n",
    "| `tensor(data,)` | 类似于`np.array`的构造函数 |\n",
    "| `ones(*sizes)` | 创建元素全为1的张量 |\n",
    "| `zeros(*sizes)` | 创建元素全为0的张量 |\n",
    "| `eye(*sizes)` | 创建单位矩阵（对角线为1，其余为0） |\n",
    "| `arange(s,e,step)` | 从s到e，步长为step |\n",
    "| `linspace(s,e,steps)` | 从s到e，均匀分布在steps步之内 |\n",
    "| `rand/randn(*sizes)` | 创建随机值/正态分布随机值的张量 |\n",
    "| `normal(mean,std)/uniform(from,to)` | 正态分布/均匀分布的随机值张量 |\n",
    "| `randperm(m)` | 创建一个0到m-1的随机排列 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "286160bddc02850d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:41.847311100Z",
     "start_time": "2024-04-15T08:58:41.813159200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
      "tensor([[0.5075, 0.0701, 0.6252],\n",
      "        [0.5206, 0.4361, 0.6957]])\n",
      "tensor([[-0.2170, -0.2150,  0.0507],\n",
      "        [ 0.4861, -0.0401, -1.9163]])\n",
      "tensor([[0.5300, 1.0972, 0.1566],\n",
      "        [0.2618, 0.6175, 0.1638]])\n",
      "tensor([2, 4, 3, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(2, 3)\n",
    "print(x)\n",
    "\n",
    "data = [[1, 2], [3, 4]]\n",
    "x = torch.tensor(data)\n",
    "print(x)\n",
    "\n",
    "x = torch.ones(2, 3)\n",
    "print(x)\n",
    "\n",
    "x = torch.zeros(2, 3)\n",
    "print(x)\n",
    "\n",
    "x = torch.eye(3)\n",
    "print(x)\n",
    "\n",
    "x = torch.arange(0, 5, 1)\n",
    "print(x)\n",
    "\n",
    "x = torch.linspace(0, 1, steps=5)\n",
    "print(x)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(x)\n",
    "y = torch.randn(2, 3)\n",
    "print(y)\n",
    "\n",
    "x = torch.normal(mean=0, std=1, size=(2, 3))\n",
    "print(x)\n",
    "\n",
    "x = torch.randperm(5)\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a06ac87e7c2ca6b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 算术操作\n",
    "\n",
    "这里由于前面到这里的张量维度不一样，我们先查看下我们的tensor维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d8fe854b7d03c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:41.981055800Z",
     "start_time": "2024-04-15T08:58:41.829814400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7dbc20f065eaa6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "显然这里不一样，因此我们需要修改 x 张量的形状为 5 x 3，并且不改变其中的数据相容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "207b04cbabe216ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:41.999810Z",
     "start_time": "2024-04-15T08:58:41.844552Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 3, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb89a4cc2b10aa7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "这里我们看到原始的张量只有5个元素，而我们需要15个元素来填充 5x3 的张量，因此我们通过 repeat 方法去填充里面原有的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "211477774653a9c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.000872100Z",
     "start_time": "2024-04-15T08:58:41.859240500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4, 3, 0, 1],\n",
       "        [2, 4, 3, 0, 1],\n",
       "        [2, 4, 3, 0, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.repeat(3,1)\n",
    "\n",
    "x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79ca6937522aca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "这里就可以用view方法进行修改了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "444835d1f61f638f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.001767400Z",
     "start_time": "2024-04-15T08:58:41.875182100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4, 3],\n",
       "        [0, 1, 2],\n",
       "        [4, 3, 0],\n",
       "        [1, 2, 4],\n",
       "        [3, 0, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(5,3)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1013f0f5dade9b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "现在来看看加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b90f64a6d4c64e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.002826Z",
     "start_time": "2024-04-15T08:58:41.889239800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 4, 3],\n",
      "        [0, 1, 2],\n",
      "        [4, 3, 0],\n",
      "        [1, 2, 4],\n",
      "        [3, 0, 1]])\n",
      "tensor([[0.6661, 0.6715, 0.1718],\n",
      "        [0.1125, 0.3767, 0.2535],\n",
      "        [0.8960, 0.9071, 0.2110],\n",
      "        [0.7836, 0.4043, 0.8065],\n",
      "        [0.8150, 0.7206, 0.2466]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.6661, 4.6715, 3.1718],\n",
       "        [0.1125, 1.3767, 2.2535],\n",
       "        [4.8960, 3.9071, 0.2110],\n",
       "        [1.7836, 2.4043, 4.8065],\n",
       "        [3.8150, 0.7206, 1.2466]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "x+y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec86a1d988d8ff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "当然，加法也具有方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41d67bb29083f58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.002826Z",
     "start_time": "2024-04-15T08:58:41.906179400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.6661, 4.6715, 3.1718],\n",
      "        [0.1125, 1.3767, 2.2535],\n",
      "        [4.8960, 3.9071, 0.2110],\n",
      "        [1.7836, 2.4043, 4.8065],\n",
      "        [3.8150, 0.7206, 1.2466]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e469793bb70572",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "也可以指定输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40d993fc805cf767",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.016727Z",
     "start_time": "2024-04-15T08:58:41.921334600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.6661, 4.6715, 3.1718],\n",
      "        [0.1125, 1.3767, 2.2535],\n",
      "        [4.8960, 3.9071, 0.2110],\n",
      "        [1.7836, 2.4043, 4.8065],\n",
      "        [3.8150, 0.7206, 1.2466]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247bcf7de4237ac1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "当然，刚才那步扩充的全部填 0 或者 1 之类的也行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ca8a1418548b483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.017815600Z",
     "start_time": "2024-04-15T08:58:41.938345Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0, 1, 2, 3, 4])\n",
    "\n",
    "zeros = torch.zeros(10)\n",
    "\n",
    "x = torch.cat((x, zeros), dim=0)\n",
    "\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2086913d5038280a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.017815600Z",
     "start_time": "2024-04-15T08:58:41.951016600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(5,3)\n",
    "\n",
    "x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94e044fa9cf28d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 索引\n",
    "\n",
    "我们还可以使用类似NumPy的索引操作来访问Tensor的一部分，需要注意的是：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。\n",
    "\n",
    "当然了，假如不知道numpy的话，索引是访问张量或数组特定元素的方式，和python列表的索引类似\n",
    "\n",
    "例如，x[0, :] 表示访问tensor x的第一行所有元素，0代表第一行， : 代表所有列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beaf6e166a5a4415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.018848300Z",
     "start_time": "2024-04-15T08:58:41.967014700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0.8441, 0.9358, 0.0616],\n",
      "        [0.8901, 0.6935, 0.3154],\n",
      "        [0.0923, 0.4046, 0.5465],\n",
      "        [0.3352, 0.4640, 0.5577],\n",
      "        [0.3268, 0.1293, 0.9693]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)\n",
    "y = x[0, :]\n",
    "\n",
    "y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e796e685ae8765a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "需要注意他们共享内存，**索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "909fc05a7697921b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.033960400Z",
     "start_time": "2024-04-15T08:58:41.983789Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6661, 0.6715, 0.1718],\n",
      "        [0.1125, 0.3767, 0.2535],\n",
      "        [0.8960, 0.9071, 0.2110],\n",
      "        [0.7836, 0.4043, 0.8065],\n",
      "        [0.8150, 0.7206, 0.2466]])\n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1.6661, 1.6715, 1.1718],\n",
      "        [1.1125, 1.3767, 1.2535],\n",
      "        [1.8960, 1.9071, 1.2110],\n",
      "        [1.7836, 1.4043, 1.8065],\n",
      "        [1.8150, 1.7206, 1.2466]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y)\n",
    "print(x)\n",
    "y += 1 \n",
    "print(y)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed3b1380c38636",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "可以注意到这里的源tensor也被修改了，那么如何断开这种内存共享关系呢？创建副本就好了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe13386f16241c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.232205300Z",
     "start_time": "2024-04-15T08:58:41.999810Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([[1., 2., 3.],\n",
      "        [3., 4., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([2., 3., 4.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [3., 4., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y)\n",
    "print(x)\n",
    "\n",
    "y = x[0, :].clone()\n",
    "y += 1\n",
    "print(y)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17304d2df5c89e0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "顺便复习下，view()方法可以改变形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a653cb0f5da6aef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.274911200Z",
     "start_time": "2024-04-15T08:58:42.015855Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "y = x.view(15)\n",
    "z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\n",
    "\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f17b23066523b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**注意view()返回的新Tensor与源Tensor虽然可能有不同的size，但是是共享data的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53e4e0ebbbdf3c3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.276911600Z",
     "start_time": "2024-04-15T08:58:42.030847300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([1., 2., 3., 4., 5., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x += 1\n",
    "print(x)\n",
    "print(y) # 也加了1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38f7f96fb146a4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "所以如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个reshape()可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用clone创造一个副本然后再使用view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fef65d32d8d333b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.278916Z",
     "start_time": "2024-04-15T08:58:42.046602200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([1., 2., 3., 4., 5., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x_cp = x.clone().view(15)\n",
    "x -= 1\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e16174f6c98375",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor。\n",
    "\n",
    "怎么理解呢，在深度学习中，计算图是一个有向图，它描述了数据（张量）如何通过操作（如加法、乘法等）相互关联，并最终产生输出。当你训练神经网络时，计算图用于反向传播过程，这个过程会计算并更新模型的权重。\n",
    "\n",
    "当你使用 .clone() 创建一个张量的副本时，这个操作会被加入到计算图中。这意味着，如果你在副本上执行了某些操作，比如计算损失并调用 .backward() 进行反向传播，梯度不仅会流向副本张量，还会继续流向原始张量。这对于保持权重更新和梯度计算的连续性非常重要。\n",
    "\n",
    "换句话说：\n",
    "\n",
    "保持梯度流动：当你对副本进行操作并进行反向传播时，原始张量也会收到来自副本的梯度信息。\n",
    "\n",
    "更新权重：如果原始张量是神经网络中的权重，这意味着它也会被更新。\n",
    "\n",
    "如果你没有使用 .clone() 而是直接通过索引创建了一个视图，那么对视图的修改会影响原始张量，但是这种修改可能不会被认为是一个操作，不会加入到计算图中，也就不会在反向传播过程中更新原始张量的梯度。\n",
    "\n",
    "对于初学者来说，你可以简单理解为：使用 .clone() 是为了确保你的计算和梯度更新都是可追踪和可控的。这在实现复杂的神经网络时至关重要，因为它确保了反向传播能够正确地更新所有相关的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db27135f0e3848b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.281924500Z",
     "start_time": "2024-04-15T08:58:42.062727800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7478])\n",
      "-0.7478055357933044\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42bca0abacf745",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "另外一个常用的函数就是item(), 它可以将一个标量Tensor转换成一个Python number："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23b1e534b5bcd7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 线性代数\n",
    "\n",
    "此外 PyTorch还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。如下表所示：\n",
    "\n",
    "\n",
    "| 函数 | 功能 |\n",
    "| --- | --- |\n",
    "| `trace` | 对角线元素之和（矩阵的迹） |\n",
    "| `diag` | 对角线元素 |\n",
    "| `triu/tril` | 矩阵的上三角/下三角，可指定偏移量 |\n",
    "| `mm/bmm` | 矩阵乘法，batch的矩阵乘法 |\n",
    "| `addmm/addbmm/addmv/addr/baddbmm..` | 矩阵运算 |\n",
    "| `t` | 转置 |\n",
    "| `dot/cross` | 点乘/叉乘 |\n",
    "| `inverse` | 矩阵逆 |\n",
    "| `svd` | 奇异值分解 |\n",
    "\n",
    "\n",
    "\n",
    "然后这里尝试用线代的方法解这道题\n",
    "\n",
    "<img src=\"./img/1-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340737301d424ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1. 首先，问题中的二次型 \\( f(x) \\) 可以用矩阵 \\( A \\) 表示为：\n",
    "\n",
    "   \\[ A = \\begin{pmatrix} 1 & a & a \\\\ a & 1 & a \\\\ a & a & 1 \\end{pmatrix} \\]\n",
    "\n",
    "   其中 \\( a \\) 是未知数。\n",
    "\n",
    "2. 通过计算 \\( A \\) 的特征值，确定 \\( a = -\\frac{1}{2} \\)，这是使 \\( A \\) 成为正定矩阵的条件（即所有特征值都是正数）。\n",
    "\n",
    "3. 接下来，我们将原始二次型 \\( f(x) \\) 通过线性变换 \\( P \\) 映射到新的二次型 \\( g(y) \\)，这里 \\( P \\) 的列是 \\( A \\)'s 特征向量，归一化后构成 \\( P \\)。\n",
    "\n",
    "4. 然后，给出了 \\( P \\) 矩阵的具体形式：\n",
    "\n",
    "   \\[ P = \\begin{pmatrix} 1/\\sqrt{3} & 1/\\sqrt{3} & 1/\\sqrt{3} \\\\ 0 & 2/\\sqrt{3} & -1/\\sqrt{3} \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 0 & 2 \\\\ 1 & -1 & 0 \\end{pmatrix} \\]\n",
    "\n",
    "5. 最后，提供了 \\( x \\) 到 \\( y \\) 的映射关系，以及 \\( P \\) 的表达式，证明这个映射是原问题的解。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a90cd9b72b30434b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.281924500Z",
     "start_time": "2024-04-15T08:58:42.080675900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征值: tensor([1.5000, 0.0000, 1.5000])\n",
      "特征向量: tensor([[ 0.8165, -0.5774,  0.3431],\n",
      "        [-0.4082, -0.5774, -0.8132],\n",
      "        [-0.4082, -0.5774,  0.4701]])\n",
      "变换矩阵P: tensor([[ 0.5774,  0.5774,  0.5774],\n",
      "        [ 0.0000,  1.1547, -0.5774],\n",
      "        [ 0.0000,  0.0000,  1.0000]])\n",
      "变换矩阵P的逆: tensor([[ 1.7321, -0.8660, -1.5000],\n",
      "        [ 0.0000,  0.8660,  0.5000],\n",
      "        [ 0.0000,  0.0000,  1.0000]])\n",
      "变换后的x值: tensor([3.4641, 0.5774, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "# 定义A矩阵\n",
    "A = torch.tensor([[1, -0.5, -0.5], [-0.5, 1, -0.5], [-0.5, -0.5, 1]], dtype=torch.float32)\n",
    "\n",
    "# 计算特征值和特征向量\n",
    "L_complex, V_complex = torch.linalg.eig(A)\n",
    "\n",
    "# 特征值实部\n",
    "L = L_complex.real\n",
    "\n",
    "# 特征向量实部\n",
    "V = V_complex.real\n",
    "\n",
    "# 定义变换矩阵P\n",
    "P = torch.tensor([\n",
    "    [1 / torch.sqrt(torch.tensor(3.0)), 1 / torch.sqrt(torch.tensor(3.0)), 1 / torch.sqrt(torch.tensor(3.0))],\n",
    "    [0, 2 / torch.sqrt(torch.tensor(3.0)), -1 / torch.sqrt(torch.tensor(3.0))],\n",
    "    [0, 0, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# 计算P的逆矩阵\n",
    "P_inv = torch.linalg.inv(P)\n",
    "\n",
    "# 打印特征值和特征向量\n",
    "print(\"特征值:\", L)\n",
    "print(\"特征向量:\", V)\n",
    "print(\"变换矩阵P:\", P)\n",
    "print(\"变换矩阵P的逆:\", P_inv)\n",
    "\n",
    "# 使用P验证变换后的y值\n",
    "y = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "x = torch.mv(P, y)\n",
    "print(\"变换后的x值:\", x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1f189c1a42d48",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "答案是这个反正\n",
    "\n",
    "\n",
    ".\n",
    "<img src=\"./img/1-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd0106734a4ab5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 广播机制\n",
    "\n",
    "前面我们看到如何对两个形状相同的Tensor做按元素运算。当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b16f54a354c1f226",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.282931500Z",
     "start_time": "2024-04-15T08:58:42.093694600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f17c41bc68dd2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "由于x和y分别是1行2列和3行1列的矩阵，如果要计算x + y，那么x中第一行的2个元素被广播（复制）到了第二行和第三行，而y中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c967cf618f8ee7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 运算的内存开销\n",
    "\n",
    "前面说了，索引操作是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。为了演示这一点，我们可以使用Python自带的id函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2785af5816e71128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.283918200Z",
     "start_time": "2024-04-15T08:58:42.109664300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "print(id(y) == id_before) # False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11a35dae866f9f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "如果想指定结果到原来的y的内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们把x + y的结果通过[:]写进y对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3991cc386be38b29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.283918200Z",
     "start_time": "2024-04-15T08:58:42.126709100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "print(id(y) == id_before) # True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74b100448b6087",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "我们还可以使用运算符全名函数中的out参数或者自加运算符+=(也即add_())达到上述效果，例如torch.add(x, y, out=y)和y += x(y.add_(x))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86acb36640264a76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.284910200Z",
     "start_time": "2024-04-15T08:58:42.142690600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "torch.add(x, y, out=y) # y += x, y.add_(x)\n",
    "print(id(y) == id_before) # True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f6fbbad7e47cad",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "注：虽然view返回的Tensor与源Tensor是共享data的，但是依然是一个新的Tensor（因为Tensor除了包含data外还有一些其他属性），二者id（内存地址）并不一致。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689b3386536221c7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Tensor和 NumPy 相互转换\n",
    "\n",
    "\n",
    "我们很容易用numpy()和from_numpy()将Tensor和NumPy中的数组相互转换。但是需要注意的一点是： 这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！\n",
    "\n",
    "还有一个常用的将NumPy中的array转换成Tensor的方法就是torch.tensor(), 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的Tensor和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2408536718717e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.285977400Z",
     "start_time": "2024-04-15T08:58:42.157198300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd64e366f9144fa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "上面是使用numpy()将Tensor转换成NumPy数组；\n",
    "\n",
    "\n",
    "下面是 使用from_numpy()将NumPy数组转换成Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f638146a0cf55d1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.285977400Z",
     "start_time": "2024-04-15T08:58:42.173042600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bec7d137ab5b16",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "所有在CPU上的Tensor（除了CharTensor）都支持与NumPy数组相互转换。\n",
    "\n",
    "此外上面提到还有一个常用的方法就是直接用torch.tensor()将NumPy数组转换成Tensor，需要注意的是该方法总是会进行数据拷贝，返回的Tensor和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b35e2affac19a7ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:42.285977400Z",
     "start_time": "2024-04-15T08:58:42.189150900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor(a)\n",
    "a += 1\n",
    "print(a, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31320cd4bf3d09",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Tensor on GPU\n",
    "\n",
    "用方法to()可以将Tensor在CPU和GPU（需要硬件支持）之间相互移动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b511f78463b33dd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:44.319400800Z",
     "start_time": "2024-04-15T08:58:42.203385400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3], device='cuda:0')\n",
      "tensor([2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2149ccef6b88a0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 自动求梯度\n",
    "\n",
    "在深度学习中，我们经常需要对函数求梯度（gradient）。PyTorch提供的autograd包能够根据输入和前向传播过程自动构建计算图，并执行反向传播。本节将介绍如何使用autograd包来进行自动求梯度的有关操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f5503fd5eb098",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "如果将 Tensor 的属性 .requires_grad 设置为 true，它将开始追踪在其上的所有操作，这样可以用链式法则进行梯度传播\n",
    "\n",
    "完成计算后，可以调用.backward()来完成所有梯度计算。此Tensor的梯度将累积到.grad属性中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8064b6993a766",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "注意在y.backward()时，如果y是标量，则不需要为backward()传入任何参数；否则，需要传入一个与y同形的Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d87c8d477be238",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "如果不想要被继续追踪，可以调用.detach()将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用with torch.no_grad()将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（requires_grad=True）的梯度。\n",
    "\n",
    "Function是另外一个很重要的类。Tensor和Function互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个Tensor都有一个.grad_fn属性，该属性即创建该Tensor的Function, 就是说该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与这些运算相关的对象，否则是None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7f4eee82def8385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:44.356066500Z",
     "start_time": "2024-04-15T08:58:44.313421500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caf17e7485f11a35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:44.358005100Z",
     "start_time": "2024-04-15T08:58:44.328319500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x1d1d6830100>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc68d1f509f450",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "注意x是直接创建的，所以它没有grad_fn, 而y是通过一个加法操作创建的，所以它有一个为<AddBackward>的grad_fn。\n",
    "\n",
    "像x这种直接创建的称为叶子节点，叶子节点对应的grad_fn是None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d07a27ed519afb8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:44.373100100Z",
     "start_time": "2024-04-15T08:58:44.342446200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(x.is_leaf, y.is_leaf) # True False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd315354b5dee6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "再试试别的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a96bc8e45065033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:44.376092400Z",
     "start_time": "2024-04-15T08:58:44.359004Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98565b2d1e0700fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "通过.requires_grad_()来用in-place的方式改变requires_grad属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de9b2e5b2bfd1bd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:44.465665200Z",
     "start_time": "2024-04-15T08:58:44.374160800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000001D1D7A69D60>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad) # True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26daee2cdd2dabb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 梯度\n",
    "\n",
    "梯度是微积分中的一个概念，它表示函数在某一点上的导数，如果函数有多个变量，梯度就是各个变量偏导数构成的向量。在深度学习中，我们通常对网络的损失函数计算梯度。这个梯度告诉我们，如果稍微改变函数的输入（在这个场景中是网络的权重），函数的输出（即损失）会如何变化。\n",
    "\n",
    "在神经网络中，我们通过梯度下降法来更新网络的权重，目的是最小化损失函数。梯度指向了损失最快增加的方向，因此，我们通过在梯度的反方向上小步移动权重，来尝试减少损失。\n",
    "\n",
    "在代码中，x 是一个张量，带有 requires_grad=True 属性，这意味着PyTorch将追踪它上面所有操作的梯度。当你对 out 调用 backward() 方法时，PyTorch会自动计算 out 关于 x 的梯度，并把它存储在 x.grad 中。\n",
    "\n",
    "因为out是一个标量，所以调用backward()时不需要指定求导变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68d6dd2d95d7833e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:44.483754300Z",
     "start_time": "2024-04-15T08:58:44.389150400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "out = z.mean()\n",
    "out.backward(retain_graph=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0992f4d0c6237",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "梯度的计算公式基于链式法则，对于 out 这个标量，它的梯度被定义为对 x 的偏导数。在你的例子中，out 是 z 的均值，而 z 又是 y 的函数，y 是 x 加2再乘以3的结果。所以，最终的梯度是这些操作组合起来对 x 的偏导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5469c051197a8f6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T08:58:44.484766100Z",
     "start_time": "2024-04-15T08:58:44.405128500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f25cd8fe1e0c80",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "可以 pip install torchviz \n",
    "\n",
    "然后根据这个教程配置环境变量，配置完后要重启pycharm和anaconda\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/74796069\n",
    "\n",
    "然后查看计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a0b73ad8708a956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T11:37:22.280136700Z",
     "start_time": "2024-04-15T11:37:22.151626900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'img\\\\1_Digraph.pdf'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# 定义一个2x2的张量x，初始化为1，并且需要计算梯度\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "\n",
    "# 对x做一个简单的运算\n",
    "y = x + 2\n",
    "\n",
    "# 再对y进行运算得到z\n",
    "z = y * y * 3\n",
    "\n",
    "# 对z取平均得到out\n",
    "out = z.mean()\n",
    "\n",
    "# 使用make_dot从out生成计算图的可视化\n",
    "dot = make_dot(out, params=dict(x=x))\n",
    "\n",
    "dot.render('./img/1_Digraph', format='png')\n",
    "\n",
    "# 展示计算图\n",
    "dot.view()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa670ae146c8b3c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "这里可以看到\n",
    "\n",
    "<img src=\"./img/1-3.png\">\n",
    "\n",
    "\n",
    "1. 最顶层的蓝色方框代表了张量 x。这是一个形状为 (2, 2) 的张量，它是进行计算和需要求梯度的原始数据。\n",
    "\n",
    "2. AccumulateGrad 表示这是一个需要梯度累加的张量。在PyTorch中，如果张量的 requires_grad 属性设置为 True，那么它会在反向传播过程中累积梯度。\n",
    "\n",
    "3. AddBackward0 是一个操作的反向传播函数，表明在前向传播中有一个加法操作 y = x + 2。这里 2 是加法操作的另一个操作数。\n",
    "\n",
    "4. 两个 MulBackward0 表示了两次乘法操作的反向传播函数。在前向传播中，先执行了 z = y * y，然后是 z = z * 3。\n",
    "\n",
    "5. MeanBackward0 是取均值操作的反向传播函数。在前向传播中，执行了 out = z.mean() 来计算 z 的均值。\n",
    "\n",
    "6. 最底层的绿色方框代表了标量输出，也就是 out。因为 out 是一个标量（单个数值），所以没有显示具体形状。\n",
    "\n",
    "整个计算图展示了从输入 x 到输出 out 的前向计算流程，以及从 out 到 x 的梯度反向传播流程。当调用 out.backward() 时，梯度会沿着这些箭头的方向流动，计算每个参与前向传播计算的张量相对于 out 的梯度。在这个过程中，x.grad 会得到更新，包含了 out 相对于 x 的偏导数。\n",
    "\n",
    "对于初学者来说，理解这张计算图的关键是将其视为说明如何从输入 x 计算输出 out 的一系列步骤，以及如何计算影响输出变化的每个输入元素的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62404650a63900b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T09:26:00.622452700Z",
     "start_time": "2024-04-15T09:26:00.601930500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.backward(retain_graph=True)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4206824136584790",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "具体的过程可以看\n",
    "\n",
    ".\n",
    "<img src=\"./img/1-4.png\">\n",
    "\n",
    "可以自己试试看手算一遍就懂了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "516db7834510940d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T11:36:10.437197Z",
     "start_time": "2024-04-15T11:36:10.385730Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x:\n",
      "tensor([[2., 3.],\n",
      "        [4., 5.]], requires_grad=True)\n",
      "Tensor y (x + 2):\n",
      "tensor([[4., 5.],\n",
      "        [6., 7.]], grad_fn=<AddBackward0>)\n",
      "Tensor z (3 * y^2):\n",
      "tensor([[ 48.,  75.],\n",
      "        [108., 147.]], grad_fn=<MulBackward0>)\n",
      "Tensor out (mean of z):\n",
      "94.5\n",
      "Gradient d(out)/dx:\n",
      "tensor([[ 6.0000,  7.5000],\n",
      "        [ 9.0000, 10.5000]])\n",
      "计算图已保存。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个2x2的张量，并设置requires_grad=True来追踪其计算历史\n",
    "x = torch.tensor([[2.0, 3.0], [4.0, 5.0]], requires_grad=True)\n",
    "\n",
    "# 执行一系列张量操作\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "# 打印出创建的张量\n",
    "print(f\"Tensor x:\\n{x}\")\n",
    "print(f\"Tensor y (x + 2):\\n{y}\")\n",
    "print(f\"Tensor z (3 * y^2):\\n{z}\")\n",
    "print(f\"Tensor out (mean of z):\\n{out}\")\n",
    "\n",
    "# 使用自动求导计算梯度\n",
    "out.backward()\n",
    "\n",
    "# 打印梯度 d(out)/dx\n",
    "print(f\"Gradient d(out)/dx:\\n{x.grad}\")\n",
    "\n",
    "# 可视化计算图\n",
    "try:\n",
    "    from torchviz import make_dot\n",
    "    dot = make_dot(out, params=dict(x=x))\n",
    "    # 保存计算图到文件\n",
    "    dot.render('./img/1_compute_graph', format='png')\n",
    "    print(\"计算图已保存。\")\n",
    "except ImportError:\n",
    "    print(\"torchviz库没有安装，无法可视化计算图。\")\n",
    "except Exception as e:\n",
    "    print(f\"计算图可视化时发生错误：{e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9cc61c4f74d58",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 总结\n",
    "\n",
    "目前来看已经会了一些 PyTorch 的基础知识，包括\n",
    "\n",
    "1. 张量操作：如何创建张量、对张量进行基本运算，以及改变张量的形状。这些是深度学习中数据操作的基本。\n",
    "2. 自动求导：PyTorch的自动求导机制，这是神经网络训练中计算梯度的关键。requires_grad标志，它可以让PyTorch追踪对张量的所有操作，并且在反向传播时自动计算梯度。\n",
    "3. 计算图和反向传播：使用 .backward() 方法实现反向传播，其会计算并更新网络的权重。计算图实际上是由节点(张量)和边(操作)构成的\n",
    "4. 梯度：它是在给定点上函数增长最快的方向。在多维空间中，梯度是由偏导数组成的向量。在神经网络中，梯度是用来指导权重更新以最小化损失函数的。\n",
    "5. 计算图可视化：使用torchviz库来可视化计算图，这有助于理解张量操作和梯度流动的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8fe9dc0b095aed55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T11:36:14.195924300Z",
     "start_time": "2024-04-15T11:36:14.108570600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'img\\\\1_compute_graph.pdf'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30fc226dfaa03c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Q: \"了解了requires_grad标志，它可以让PyTorch追踪对张量的所有操作，并且在反向传播时自动计算梯度。\" 这里我还是有点不太理解，请讲解说明\n",
    "\n",
    "A: 在PyTorch中，张量（Tensors）是神经网络中数据的基本组成部分。当设置了 requires_grad=True，PyTorch会开始追踪在这个张量上进行的所有操作（operation）。这意味着PyTorch会记录下操作的历史，以便稍后在反向传播过程中计算梯度。\n",
    "\n",
    "A: 这个特性是深度学习中的自动微分的核心，它允许神经网络通过梯度下降算法来优化参数。在反向传播过程中，PyTorch通过计算图回溯，自动计算并存储每个 requires_grad=True 的张量的梯度。\n",
    "\n",
    "A: 下面是一个代码例子，其中 x 是我们希望优化的参数，我们计算它的一些函数的输出 out，然后通过调用 out.backward() 来计算梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd843fc33ef9227c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T11:39:57.241808200Z",
     "start_time": "2024-04-15T11:39:57.193289400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out: 54.0\n",
      "Gradient d(out)/dx: tensor([ 4.,  6.,  8., 10.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量x，设置requires_grad=True以跟踪其计算历史\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "\n",
    "# 定义一个张量上的操作，比如一个多项式函数\n",
    "y = x**2 + 2*x + 1\n",
    "\n",
    "# 现在我们对y进行求和操作，得到一个标量输出\n",
    "out = y.sum()\n",
    "\n",
    "# 打印输出值\n",
    "print(f\"Out: {out}\")\n",
    "\n",
    "# 现在我们执行反向传播，PyTorch会自动计算梯度\n",
    "out.backward()\n",
    "\n",
    "# 打印梯度 d(out)/dx\n",
    "print(f\"Gradient d(out)/dx: {x.grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a564f8290e90cf8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "在上面的例子中，out 是 y 的总和。因为 out 是一个标量，所以调用 out.backward() 时不需要为其传递任何参数。PyTorch会计算 out 相对于所有 requires_grad=True 的张量（在这个例子中是 x）的梯度。在执行 out.backward() 之后，x.grad 会包含 out 相对于 x 的梯度。\n",
    "\n",
    "<img src=\"./img/1-5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd4000d347e818fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T11:43:04.568018Z",
     "start_time": "2024-04-15T11:43:04.552488200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  6.,  8., 10.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad\n",
    "\n",
    "# y.grad # 会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c341d839d210dcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T11:43:51.546530800Z",
     "start_time": "2024-04-15T11:43:51.532744500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(x.is_leaf, y.is_leaf) # True False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfc5f3f7ba88c1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Q: 另外是不是只能对叶子tensor求梯度，我刚试了下 y就没有 y.grad\n",
    "\n",
    "A: 梯度是默认累积到叶子张量上的，叶子张量是指那些直接从数据创建的张量，它们的 requires_grad 属性被设置为 True。这些张量通常用于模型参数。\n",
    "\n",
    "A: 当你执行计算并通过 .backward() 调用进行反向传播时，只有叶子张量的 .grad 属性会被填充。中间张量（即那些作为操作结果创建的张量，例如 y）的梯度默认是不保留的，因为这些梯度在大多数情况下不需要，且会占用大量内存。\n",
    "\n",
    "A: 如果你希望访问非叶子张量的梯度，你可以使用 hook 方法来保留它。这是通过注册一个钩子（hook）函数来实现的，它将在反向传播过程中被调用。下面是一个如何注册钩子以保存 y 的梯度的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60b080ef3309b32d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T11:44:58.760390800Z",
     "start_time": "2024-04-15T11:44:58.709866700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y: tensor([2., 2., 2., 2.])\n",
      "Gradient of x: tensor([ 4.,  8., 12., 16.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量x\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "\n",
    "# 计算y\n",
    "y = x ** 2\n",
    "\n",
    "# 为了获取非叶子张量的梯度，我们使用.register_hook\n",
    "# 这里我们打印y的梯度\n",
    "hook = y.register_hook(lambda grad: print(f\"Gradient of y: {grad}\"))\n",
    "\n",
    "# 计算z\n",
    "z = y * 2\n",
    "\n",
    "# 对z求和得到out\n",
    "out = z.sum()\n",
    "\n",
    "# 执行反向传播\n",
    "out.backward()\n",
    "\n",
    "# 移除hook\n",
    "hook.remove()\n",
    "\n",
    "# 由于x是叶子节点，它的梯度被保存在x.grad中\n",
    "print(f\"Gradient of x: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec1da7b1d6a6c94",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "在这个代码示例中，y 不是叶子张量，因为它是由 x 经过操作得来的。通过使用 .register_hook() 方法，我们能够打印出在反向传播过程中 y 的梯度。\n",
    "\n",
    "注意：通常情况下，我们并不需要手动操作这些中间梯度，除非你正在进行更高级的操作，比如实现自定义的梯度更新算法。在大多数深度学习应用中，只有叶子节点的梯度会被用来更新模型参数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
